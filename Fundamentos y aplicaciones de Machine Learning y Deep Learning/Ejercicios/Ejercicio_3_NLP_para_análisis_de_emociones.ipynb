{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.3.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhFF63rsMfbI"
      },
      "source": [
        "# Ejercicio 3 - Análisis de emociones en texto no-estructurado (NLP)\n",
        "\n",
        "La idea es realizar un análisis de emociones desde comentarios en medios sociales, por medio de un modelo de clasificación supervisada, que entrega una de 3 posibles clases de emoción.\n",
        "\n",
        "## Contexto: Análisis de texto de comentarios en medios sociales de diversos países (Chile, Arg, Mex, otros)\n",
        "\n",
        "Este conjunto de datos generado en 2016 por [R:Solver](http://rsolver.com) y compartido parcialmente para este tipo de ejercicios, consiste en dos columnas: el texto del comentario, y una clasificación dentro de las tres alternativas o clases de emoción.\n",
        "\n",
        "El gran objetivo final a resolver con este ejemplo, es analizar una porción de texto y lograr predecir la emoción correspondiente, dentro de 3 posibles clases:\n",
        "\n",
        "*     Enojo\n",
        "*     Sorpresa\n",
        "*     Alegría\n",
        "\n",
        "**CONSIDERACIÓN IMPORTANTE**\n",
        "\n",
        "Se sabe que para lograr buenos resultados se requiere contar con muchos miles de ejemplos y en este ejercicio académico sólo se cuenta con pocos miles por cada clase. En la práctica se podrá apreciar que el desempeño general (*accuracy*) de los modelos es mediano-bajo (cerca del 50%). Esto, seguramente, podría mejorar recién al contar con varias decenas de miles de ejemplos adicionales a los actuales, lo que se explica por la gran diversidad de terminología encontrada en estos comentarios (son cerca de 20.000 términos o palabras diferentes en este corpus).\n",
        "\n",
        "Por esta razón, el foco está en realizar análisis sobre estos desempeños y sacar conclusiones útiles, hasta cierto punto independiente de los *accuracies* correspondientes. Este ejercicio busca reforzar conceptos desde la perspectiva de quienes ya están adentrándose más en entender el funcionamiento de modelos de clasificación supervisada.\n",
        "\n",
        "---\n",
        "\n",
        "## Instrucciones Generales\n",
        "\n",
        "Se deben contestar las preguntas que se indican en las secciones de \"Preguntas\", más adelante. Se puede recurrir a ejercicios de otras fuentes, así como al material de clases.\n",
        "\n",
        "La entrega se realiza en forma de un informe en formato PDF, utilizando la plantilla de informe que está en http://dcc.rsolver.com/dcc/docs/InformeActividad.docx\n",
        "\n",
        "Esta entrega se puede subir por un único miembro del grupo a la tarea o registro en el portal del curso.\n",
        "\n",
        "En caso de no poder subirlo por esa vía o que no esté habilitado, se puede usar el buzón alternativo: http://aiker.rsolver.com/aiker/DocUpload.aspx (*)\n",
        "\n",
        "(*) Si y solo si hay problemas en la carga, enviar el PDF a rsandova@ing.puc.cl y cc: ayudante@aiker.ai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbsvny1wYwsT"
      },
      "source": [
        "## Paso 1: Instalar librerías de modelos de clasificación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr8D6ajXY2T9"
      },
      "source": [
        "install.packages('e1071')\n",
        "install.packages('caret')\n",
        "install.packages('caTools')\n",
        "install.packages('tm')\n",
        "install.packages('rpart')\n",
        "install.packages('SnowballC')\n",
        "install.packages('nnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q3bYb0RM5bx"
      },
      "source": [
        "## Paso 2: Carga de los datos\n",
        "\n",
        "La siguiente celda de código carga los datos desde la URL de origen y luego muestra un encabezado con las primeras filas del dataset, para demostrar la disposición y ejemplos de los datos.\n",
        "\n",
        "**Nótese que hay un desbalance entre los ejemplos de cada una de las 3 clases, pero NO se espera ni se pide modificar esto, lo cual queda a criterio de los alumnos experimentar con re-balancear las clases y ver su desempeño.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIIKPnj-IRE8"
      },
      "source": [
        "# Se declara la URL de dónde obtener los datos\n",
        "theUrlMain <- \"http://RAlize.RSolver.com/RAlize/data/small_emotion_sample_2016.csv\"\n",
        "\n",
        "# Se declaran los nombres de las columnas\n",
        "columnas <- c(\"texto\",\"emoción\")\n",
        "\n",
        "# Se cargan datos principales a una estructura (commentsdataset), asignando nombres de atributos a las columnas\n",
        "comments.dataset.raw <- read.csv(file = theUrlMain, header = FALSE, sep = \";\", col.names=columnas, skipNul = TRUE)\n",
        "\n",
        "# Conteo de ejemplos de cada clase.\n",
        "# Al ver su dimensión, se puede apreciar si están o no balanceados,\n",
        "# pero no se espera en esta ocasión trabajar en balanceo de clases.\n",
        "data.enojo <- comments.dataset.raw[comments.dataset.raw$emoción == 'enojo',]\n",
        "data.sorpresa <- comments.dataset.raw[comments.dataset.raw$emoción == 'sorpresa',]\n",
        "data.alegria <- comments.dataset.raw[comments.dataset.raw$emoción == 'alegria',]\n",
        "cat(\"\\nEjemplos Enojo:     \", dim(data.enojo))\n",
        "cat(\"\\nEjemplos Sorpresa:  \", dim(data.sorpresa))\n",
        "cat(\"\\nEjemplos Alegría:   \", dim(data.alegria))\n",
        "\n",
        "commentsdataset <- comments.dataset.raw\n",
        "dim(commentsdataset)\n",
        "head(commentsdataset, 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGOO7LFAPDL5"
      },
      "source": [
        "## Ejercicio 1: Entender el efecto de la normalización de texto\n",
        "\n",
        "Las técnicas de normalización de texto, cuando tienen que realizar análisis del texto leído, deben buscar la simplificación del texto para poder trabajar sobre un universo de términos más simple y acotado. Esto puede considerar eliminar tildes, evitar palabras comunes, usar la raíz de múltiples términos, entre otros.\n",
        "\n",
        "A continuación se aplican algunas de las técnicas más frecuentes, permitiendo ver el efecto (potencialmente positivo) de cada una de ellas.\n",
        "\n",
        "**Pregunta 1.1 (1 punto)**: ¿Cuánto mejora el rendimiento de los modelos de análisis de sentimiento (implementados en la siguiente sección) al agregar las técnicas de normalización? La idea es comparar el rendimiento con y sin las técnicas de normalización, analizando cuantitativamente las posibles mejoras en cada uno de los dos modelos (a nivel de sus indicadores de desempeño).\n",
        "\n",
        "**Pregunta 1.2 (0,5 puntos)**: ¿Cuál es el modelo que mejora más? Analice y describa qué característcas de este modelo justifican ser el que mejora con las técnicas de normalización, más que el otro.\n",
        "\n",
        "Aquí se recomienda registrar el desempeño de los modelos habiendo ejecutado con y sin las técnicas de normalización. Para verificar el desempeño SIN la normalización se puede eliminar las 5 instrucciones o técnicas que están el código como Bloque 1 (se recomienda comentar la línea de código respectivo con un símbolo #).\n",
        "\n",
        "Se debe comparar el desempeño de ambos modelos, con y sin las normalizaciones de texto, de modo de ver en cuál de ellos tiene más efecto. Para efectos prácticos, se propone realizar la modificación en el BLOQUE 1 y correr todas las secciones de código en adelante, para ver el rendimiento comparado.  \n",
        "\n",
        "Luego de contestar estas preguntas, continue con el ejercicio dejando activas las técnicas de normalización en adelante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-RkhmupJGPI"
      },
      "source": [
        "library(tm)\n",
        "library(SnowballC)\n",
        "\n",
        "# Construye el Corpus: el universo de texto que se usará para entrenar los modelos.\n",
        "corpus.original <- Corpus(VectorSource(commentsdataset$texto))\n",
        "\n",
        "# Se selecciona y muestra (sin normalizar) un comentario de ejemplo aleatorio dentro del corpus\n",
        "random_index <- floor(runif(1, min=0, max=length(corpus.original)))\n",
        "content(corpus.original[[random_index]])\n",
        "\n",
        "################################\n",
        "# NORMALIZACIÓN DEL TEXTO\n",
        "# EJERCICIO: probar comentando (anteponiendo #) estas acciones,\n",
        "# para ver cuánto es el efecto en el rendimiento del modelo de clasificación\n",
        "################################\n",
        "# Se saca una copia ('corpus') de trabajo, para no alterar el original\n",
        "corpus <- corpus.original\n",
        "\n",
        "## BLOQUE 1\n",
        "# Se pasan todas las palabras a minúsculas\n",
        "corpus <- tm_map(corpus, tolower)\n",
        "# Se eliminan todos los signos de puntuación\n",
        "corpus <- tm_map(corpus, removePunctuation)\n",
        "# Se eliminan todos los números\n",
        "corpus <- tm_map(corpus, removeNumbers)\n",
        "# Se eliminan las stop words (palabras comunes, irrelevantes)\n",
        "corpus <- tm_map(corpus, removeWords, c(stopwords(\"spanish\")))\n",
        "# Se lleva cada palabra a su raíz (stemming)\n",
        "corpus <- tm_map(corpus, stemDocument)\n",
        "\n",
        "\n",
        "# Se muestra el mismo ejemplo aleatorio, pero en texto normalizado\n",
        "content(corpus[[random_index]])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m6AjrGNtf3x"
      },
      "source": [
        "## Ejercicio 2: Construcción de un Vocabulario con Términos Significativos\n",
        "\n",
        "Los clasificadores reciben un X de entrada de una dimensión fija. Por lo tanto los X de este ejemplo de análisis de texto, comentarios de cantidad variable de palabras, no se pueden usar tal cual vienen (como lista de cantidad variables de términos).\n",
        "\n",
        "Por ello, la siguiente porción de código transforma los comentarios de texto variable en un vector de ocurrencia de palabras referenciando un vocabulario, el cual se construye referenciando todas las palabras distintas (ya normalizadas) del Corpus. Esto se traduce en que un comentario que tiene la expresión \".. resultados excelentes ... \", tendría una intersección con el comentario \"... excelente como resultó ...\" y por ello podrían ser interpretadas en forma equivalente.\n",
        "\n",
        "Pero el desafío de esta vectorización en base a un vocabulario es la **cantidad de dimensiones** (cantidad de palabras diferentes). Un vocabulario perfectamente puede tener varios miles de palabras diferentes (ver resultado en 2.A), entonces la dimensión del vector X es de esos varios miles.\n",
        "\n",
        "Esto motiva a reducir la dimensionalidad del problema (el tamaño del vocabulario) al reconocer cuáles son los términos más relevantes. Esto se hace con removeSparseTerm() y un umbral alto (0.995 reduce la gran cantidad de términos que tienen al menos un 99.5% de 0s en la columna), como se ve en en 2.B. Mientras mayor es el número, mayor cantidad de términos ocasionales o esporádicos se conservan y el vocabulario queda más grande. En otras palabras, mientras menor es el número, mayor es la exigencia para un término de ser considerado valioso y quedar como parte del dataset. (https://www.rdocumentation.org/packages/tm/versions/0.7-8/topics/removeSparseTerms)\n",
        "\n",
        "La pregunta específica a responder cuando se implementa un modelo es: ¿cuántos son los términos a considerar en el vocabulario que sean los mejores representativos del universo de diversas palabras, para maximizar el desempeño del modelo de clasificación? Eso se responde buscando un factor de removeSparseTerms() que logre ese mejor desempeño.\n",
        "\n",
        "**Pregunta 2.1 (1 punto)** ¿Cuál es el valor usado en removeSparseTerm() para lograr mejores resultados en los modelos de clasificación? Realice una comparación de 3 valores: 0.990, 0.995, 0.998.\n",
        "\n",
        "**Pregunta 2.2 (1 punto)** ¿Cómo se interpreta en este caso que mejore o empeore con más y a veces con menos cantidad de palabras (o columnas del dataset)?\n",
        "\n",
        "Para ambas preguntas se debe comparar los resultados del Árbol de Decisión y de la Red Neuronal (utilizando parámetros de ejecución más liviana - ej: size=20 y maxit=30).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3uvXCD-MdZr"
      },
      "source": [
        "#######################################################\n",
        "# Indexación de términos: creación de un Vocabulario\n",
        "#######################################################\n",
        "\n",
        "# Primero una matriz de ocurrencia de términos o palabras (DTM: Document-term matrix).\n",
        "# Las filas son los comentarios y las columnas son las palabras diferentes encontradas (varios miles).\n",
        "termMatrix <- DocumentTermMatrix(corpus)\n",
        "dim(termMatrix)   # Resultado 2.A -> Las columnas son todos los términos diferentes encontrados en corpus\n",
        "\n",
        "# Entonces, se eliminan las palabras menos relevantes (sparse terms: términos dispersos)\n",
        "# lo que resulta en una reducción dimensional (potencialmente grande)\n",
        "# Ejercicio 2.1: ¿qué factor de eliminación (en rango 0.99x) da mejores resultados?\n",
        "# Probar con 3 combinaciones: 0.990, 0.995, 0.998.\n",
        "# Ejercicio 2.2: ¿qué implica o cómo se interpreta ese cambio de valor y su efecto en el dataset?\n",
        "termMatrixLight <- removeSparseTerms(termMatrix, 0.995)\n",
        "dim(termMatrixLight)  # Resultado 2.B -> Se puede ver que se reduce significativamente la cantidad de columnas (palabras)\n",
        "\n",
        "# Re-formatea como un DataFrame\n",
        "corpus.procesado <- as.data.frame(as.matrix(termMatrixLight))\n",
        "\n",
        "# Se construye el dataset para entrenamiento, recuperando el comentario original, sin procesar\n",
        "# agregando la columna \"emoción\"\n",
        "corpus.procesado$emoción <- as.factor(commentsdataset$emoción)\n",
        "\n",
        "# Se muestra el vocabulario y su tamaño\n",
        "dim(corpus.procesado)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gze81qto-1oX"
      },
      "source": [
        "## Paso 3: Ejecución modelos de predicción según conjuntos de entrenamiento y validación\n",
        "\n",
        "En este caso, el dataset de ejemplos etiquetados se divide en dos (Hold-out) para entrenar y validar con conjuntos disjuntos.\n",
        "\n",
        "No se necesita modificar esta sección, aunque es válido y potencialmente interesante revisar diferentes valores de proporción entrenamiento/validación.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1j2-rNWMcnt"
      },
      "source": [
        "library(caTools)\n",
        "\n",
        "################################################################\n",
        "# Versión simple para crear conjuntos de entrenamiento y de test.\n",
        "# Se puede dejar la proporción original de 0.70,\n",
        "# pero se puede cambiar si se piensa que puede hacer una diferencia.\n",
        "ratio <- sample(nrow(corpus.procesado),nrow(corpus.procesado)*0.70)\n",
        "training.set = corpus.procesado[ratio,]\n",
        "testing.set  = corpus.procesado[-ratio,]\n",
        "\n",
        "cat(\"Dimensiones Entrenamiento/Test\")\n",
        "dim(training.set)\n",
        "dim(testing.set)\n",
        "\n",
        "head(testing.set, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82LcskQOnNmH"
      },
      "source": [
        "## Ejercicio 3: Interpretación de desempeño de modelos de clasificación de referencia\n",
        "\n",
        "Habiendo definido y establecido los conjuntos de entrenamiento y de test, a continuación se ejecutan dos diferentes modelos de clasificación: Árbol de Decisión y más abajo una Red Neuronal (NNET). Cada uno obtiene su resultado, mostrando sus indicadores de desempeño.\n",
        "\n",
        "Nótese que ninguno de los 2 modelos da resultados significativamente buenos o mucho mejor que el otro, lo cual se debe a la inherente complejidad de los casos de lenguaje natural y contar sólo con unos pocos miles de ejemplos (en total son cerca de 7.000 ejemplos, divididos en 3 clases, lo cual no alcanza a reflejar la real diversidad de palabras asociadas a cada una de las clases, cosa que se empieza a lograr desde las decenas de miles de ejemplos por clase).\n",
        "\n",
        "Sin embargo, hay algunas observaciones que salen de los resultados de desempeño, además del hecho de que la red neuronal toma cerca de 10x el tiempo del Árbol de Decisión. De estas observaciones, se pide responder lo siguiente.\n",
        "\n",
        "**Pregunta 3.1 (1 punto)** Considere cambiar los parámetros de funcionamiento de la red neuronal (size, maxit) buscando su mejor desempeño e indique los valores de estos dos parámetros que resultó en mejores resultados.\n",
        "\n",
        "**Pregunta 3.2 (0,7 punto)** Teniendo este resultado se ve que de los dos modelos, hay uno con *accuracy* mejor que el otro, pero en forma más completa y precisa ¿cuál de los dos se puede considerar el mejor modelo al mirar su desempeño más completo y por qué? Nótese que la tabla de resultados tiene una serie de indicadores diferentes por cada una de las 3 clases, que sirven para entender cómo se comporta el modelo en una correcta clasificación.\n",
        "\n",
        "**Pregunta 3.3 (0,8 punto)** Se puede observar que el Árbol de Decisión tiene un muy mal desempeño en reconocer una de esas clases. ¿Cuál y por qué podría darse esto? (En otras palabras, ¿qué característica de este modelo puede provocar que en este caso una de las clases no sea factible de predecir?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzgDdw-S3sAg"
      },
      "source": [
        "**Árbol de Decisión**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOpqb-QInkff"
      },
      "source": [
        "library(caret)\n",
        "library(rpart)\n",
        "\n",
        "DT_model <- rpart(emoción ~ ., data=training.set, method=\"class\", minbucket=20)\n",
        "\n",
        "DT_predictTraining <- predict(DT_model, training.set, type = \"class\")\n",
        "DT_predictTesting <- predict(DT_model, testing.set, type = \"class\")\n",
        "cat(\"\\n\\n************* Resultados Árbol de Decisión - Testing *************\\n\")\n",
        "confusionMatrix(DT_predictTesting, as.factor(testing.set$emoción))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e4KsCJc36u-"
      },
      "source": [
        "**Red Neuronal**\n",
        "\n",
        "Nótese que toma varios minutos la ejecución de entrenamiento y evaluación de esta red, por lo que se recomienda hacer uso estratégico de los cambios en la sección/ejercicio 1 y 2 anteriores antes de probar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVsGNIiwceZ-"
      },
      "source": [
        "library(nnet)\n",
        "\n",
        "# Parámetros de nnet():\n",
        "#     size: moverse entre 5 y 40. Es la cantidad de nodos de la única capa intermedia.\n",
        "#           Sin embargo, nótese que según el tamaño del vocabulario,\n",
        "#           valores altos de size pueden resultar en error de ejecución.\n",
        "#           En estos casos, sólo queda reducir el valor de size hasta que MaxNWts quede en menos de 30.000.\n",
        "#     maxit: 20 para pruebas cortas, hasta más de 100 (varios minutos de ejecución)\n",
        "#     MaxNWts: cantidad tope de arcos de conexión. 30.000 se mantiene dentro del tope de RAM,\n",
        "#              pero puede requerir varios minutos de ejecución.\n",
        "cat(\"Entrenando Red Neuronal\\n\")\n",
        "NN.model <- nnet(as.factor(emoción) ~ ., data=training.set, size=100, maxit=50, MaxNWts=30000)\n",
        "NN.predict <- predict(NN.model, testing.set, type=\"class\")\n",
        "\n",
        "cat(\"Resultados Red Neuronal\\n\")\n",
        "x <- testing.set[, 1:dim(corpus.procesado)[2]-1]\n",
        "y <- testing.set[, dim(corpus.procesado)[2]]\n",
        "y.predicted <- predict(NN.model, x, type = 'class')\n",
        "confusionMatrix(as.factor(y.predicted), y)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}