{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jN2ucok879H"
   },
   "source": [
    "# Ejercicio complementario 1: Clasificación XGBoost\n",
    "\n",
    "Este ejercicio ilustrativo utiliza un modelo XGBoost, de la familia de los Gradient Boosting, para realizar una clasificación de tipos de flores, utilizando el dataset de flores **Iris** https://www.kaggle.com/datasets/vikrishnan/iris-dataset\n",
    "\n",
    "El objetivo de este ejercicio es simplemente conocer la forma de ejecutar un modelo XGBoost, incluyendo la preparación de los dataset para entrenamiento y evaluación.\n",
    "\n",
    "**INSTRUCCIONES**\n",
    "\n",
    "Todos los alumnos pueden utilizar las preguntas que se indican en las secciones de \"Preguntas\", más adelante, como ejercicio de comprensión de este modelo de clasificación. Se puede recurrir a ejercicios de otras fuentes, así como al material de clases.\n",
    "\n",
    "Ejercicio complementario: sin nota."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbsvny1wYwsT"
   },
   "source": [
    "## Paso 1: Instalar las librerías de modelos de clasificación\n",
    "\n",
    "Esto se ejecuta sólo una vez al comienzo de la sesión de cada persona. No se necesita volver a ejecutar con cada nueva prueba del resto de los scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yr8D6ajXY2T9"
   },
   "outputs": [],
   "source": [
    "# Esto toma cerca de 15 minutos. Paciencia\n",
    "install.packages('xgboost')\n",
    "install.packages('caret')\n",
    "install.packages('e1071')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISGXDxVfKYAF"
   },
   "source": [
    "## Paso 2: Preparación de los datasets\n",
    "\n",
    "Tomando como base el dataset Iris, se conforman conjuntos para entrenamiento y evaluación. En particular esta versión del modelo XGBoost requiere datasets en un formato de matriz, lo que requiere la conversión del dataset original, a las matrices correspondientes. Nótese que se separan los atributos (primeras columnas, excepto la final) y la etiqueta, en este caso, la especie de flor (columna final)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MdvGNZtPep_"
   },
   "outputs": [],
   "source": [
    "library(caret)\n",
    "library(e1071)\n",
    "library(xgboost)\n",
    "\n",
    "# Primero se obtiene el dataset original y se muestran sus características\n",
    "data <- iris\n",
    "head(data)\n",
    "summary(data)\n",
    "dim(data)\n",
    "\n",
    "# Segundo, se hace una partición para armar conjunto de entrenamiento y de evaluación\n",
    "# createDataPartition() es una función del paquete caret\n",
    "# Ejercicio 1: probar diferentes distribuciones (0.6, 0.7, 0.8, 0.9) para ver cuál logra mejor desempeño.\n",
    "parts = createDataPartition(data$Species, p = 0.7, list = F)\n",
    "train = data[parts, ]\n",
    "test = data[-parts, ]\n",
    "\n",
    "# Tercero, conversión de los conjuntos a formato matriz\n",
    "X_train = data.matrix(train[,-5])\n",
    "y_train = train[,5]\n",
    "\n",
    "X_test = data.matrix(test[,-5])\n",
    "y_test = test[,5]\n",
    "\n",
    "xgboost_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "xgboost_test = xgb.DMatrix(data=X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZezyZKClMUkI"
   },
   "source": [
    "## Paso 3: Ejecución de la clasificación\n",
    "\n",
    "Contando ya con los datasets de entrenamiento y evaluación, se entrena un modelo XGBoost.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9qkHfOnLYlP"
   },
   "outputs": [],
   "source": [
    "library(xgboost)\n",
    "\n",
    "# Ejercicio 2: probar diferentes valores de max.depth y nrounds\n",
    "# para ver si se mejora (o empeora) el desempeño del modelo.\n",
    "model <- xgboost(data = xgboost_train,                    # el dataset de entrenamiento\n",
    "                 max.depth=3, ,                           # profundidad máxima de los árboles\n",
    "                 nrounds=50)                              # máxima cantidad de iteraciones de entrenamiento\n",
    "summary(model)\n",
    "\n",
    "pred_test = predict(model, xgboost_test)\n",
    "pred_test\n",
    "\n",
    "pred_test[(pred_test>3)] = 3\n",
    "pred_y = as.factor((levels(y_test))[round(pred_test)])\n",
    "print(pred_y)\n",
    "\n",
    "conf_mat = confusionMatrix(y_test, pred_y)\n",
    "print(conf_mat)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
