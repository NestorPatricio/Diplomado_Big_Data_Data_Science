{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6H3ATAdp_URp"
   },
   "source": [
    "# Clasificación de imágenes con Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ABTO7f7lACL"
   },
   "source": [
    "## Obtener las clases a predecir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlx6-LFL_jbi"
   },
   "source": [
    "Descargamos sólo un subconjunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XXv-xzU1sd88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-29 21:25:32--  https://raw.githubusercontent.com/zaidalyafeai/zaidalyafeai.github.io/master/sketcher/mini_classes.txt\n",
      "Resolviendo raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Conectando con raw.githubusercontent.com (raw.githubusercontent.com)[185.199.110.133]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 404 Not Found\n",
      "2023-05-29 21:25:32 ERROR 404: Not Found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://raw.githubusercontent.com/zaidalyafeai/zaidalyafeai.github.io/master/sketcher/mini_classes.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GL_TdMffD6-"
   },
   "source": [
    "Leemos el archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eP-OxOx5sy0b"
   },
   "outputs": [],
   "source": [
    "f = open(\"mini_classes.txt\",\"r\")\n",
    "# And for reading use\n",
    "classes = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTE6D3uxtMc5"
   },
   "outputs": [],
   "source": [
    "classes = [c.replace('\\n','').replace(' ','_') for c in classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvFclytBlaA3"
   },
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NDfBHVjACAt"
   },
   "source": [
    "# Descargamos el dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MC_PUS-fKjH"
   },
   "source": [
    "Descargamos sólo los datos correspondientes a las clases que tenemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdSUnpL0u22Q"
   },
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22DPhL5FtWcQ"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "def download():\n",
    "  \n",
    "  base = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
    "  for c in classes:\n",
    "    cls_url = c.replace('_', '%20')\n",
    "    path = base+cls_url+'.npy'\n",
    "    print(path)\n",
    "    urllib.request.urlretrieve(path, 'data/'+c+'.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5jF6TXXu-Bu"
   },
   "outputs": [],
   "source": [
    "download() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEdnbBVXAI-X"
   },
   "source": [
    "# Importamos las librerías a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2FYrPgOKh6t"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6o30ipBPAQ5Y"
   },
   "source": [
    "# Cargamos los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBq3GXEKAYuO"
   },
   "source": [
    "Cada clase contiene diferente número de ejemplos almacenados en formato .npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6HEIgQNHYQnl"
   },
   "outputs": [],
   "source": [
    "def load_data(root, vfold_ratio=0.2, max_items_per_class= 4000 ):\n",
    "    all_files = glob.glob(os.path.join(root, '*.npy'))\n",
    "\n",
    "    #inicializamos las variables\n",
    "    x = np.empty([0, 784])\n",
    "    y = np.empty([0])\n",
    "    class_names = []\n",
    "\n",
    "    #cargamos cada archivo de datos\n",
    "    for idx, file in enumerate(all_files):\n",
    "        data = np.load(file)\n",
    "        data = data[0: max_items_per_class, :]\n",
    "        labels = np.full(data.shape[0], idx)\n",
    "\n",
    "        x = np.concatenate((x, data), axis=0)\n",
    "        y = np.append(y, labels)\n",
    "\n",
    "        class_name, ext = os.path.splitext(os.path.basename(file))\n",
    "        class_names.append(class_name)\n",
    "\n",
    "    data = None\n",
    "    labels = None\n",
    "    \n",
    "    #generamos el dataset de forma aleatoria\n",
    "    permutation = np.random.permutation(y.shape[0])\n",
    "    x = x[permutation, :]\n",
    "    y = y[permutation]\n",
    "\n",
    "    #separamos en entrenamiento y pruebas\n",
    "    vfold_size = int(x.shape[0]/100*(vfold_ratio*100))\n",
    "\n",
    "    x_test = x[0:vfold_size, :]\n",
    "    y_test = y[0:vfold_size]\n",
    "\n",
    "    x_train = x[vfold_size:x.shape[0], :]\n",
    "    y_train = y[vfold_size:y.shape[0]]\n",
    "    return x_train, y_train, x_test, y_test, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6uUjN-WL2Y9"
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, class_names = load_data('data')\n",
    "num_classes = len(class_names)\n",
    "image_size = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhGEDS0SMgLK"
   },
   "outputs": [],
   "source": [
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNZmQvBWBBHE"
   },
   "source": [
    "Vemos algún dato de forma aleatoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KfpDaHRkyMQC"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "%matplotlib inline  \n",
    "idx = randint(0, len(x_train))\n",
    "plt.imshow(x_train[idx].reshape(28,28)) \n",
    "print(class_names[int(y_train[idx].item())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8InHz5NBFrV"
   },
   "source": [
    "# Preprocesamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2GHUq7D2r9e"
   },
   "outputs": [],
   "source": [
    "# Hacemos reshape y normalizamos los datos\n",
    "x_train = x_train.reshape(x_train.shape[0], image_size, image_size, 1).astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], image_size, image_size, 1).astype('float32')\n",
    "\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0\n",
    "\n",
    "# Convertimos las clases desde vectores a matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rL6XAb4hBMSc"
   },
   "source": [
    "# Creamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYUVV2wf2z8H"
   },
   "outputs": [],
   "source": [
    "# Definimos el modelo\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Convolution2D(16, (3, 3),\n",
    "                        padding='same',\n",
    "                        input_shape=x_train.shape[1:], activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Convolution2D(32, (3, 3), padding='same', activation= 'relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Convolution2D(64, (3, 3), padding='same', activation= 'relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size =(2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(100, activation='softmax')) \n",
    "\n",
    "adam = tf.compat.v1.train.AdamOptimizer()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['top_k_categorical_accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YRSRkOyBP1P"
   },
   "source": [
    "# Hacemos el entrenamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7OMEJ7kF3lsP"
   },
   "outputs": [],
   "source": [
    "model.fit(x = x_train, y = y_train, validation_split=0.1, batch_size = 256, verbose=2, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2KztY7qEn9_"
   },
   "source": [
    "# Pruebas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssaZczS7DxeA"
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Accuracy: {:0.2f}%'.format(score[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xBM_w0VBbNr"
   },
   "source": [
    "# Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nH3JfoiYHdpk"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "%matplotlib inline  \n",
    "idx = randint(0, len(x_test))\n",
    "img = x_test[idx]\n",
    "plt.imshow(img.squeeze()) \n",
    "pred = model.predict(np.expand_dims(img, axis=0))[0]\n",
    "ind = (-pred).argsort()[:5]\n",
    "latex = [class_names[x] for x in ind]\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPp5D82YBhM-"
   },
   "source": [
    "# Almacenamos las clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NoFI1msFYpCN"
   },
   "outputs": [],
   "source": [
    "with open('class_names.txt', 'w') as file_handler:\n",
    "    for item in class_names:\n",
    "        file_handler.write(\"{}\\n\".format(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfJ6dpaDBpRx"
   },
   "source": [
    "# Instalamos TensorFlowJS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJJDfp9mY9Xh"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflowjs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oBl0ZKVB00d"
   },
   "source": [
    "# Guardamos y convertimos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVICB3TbZGb2"
   },
   "outputs": [],
   "source": [
    "model.save('keras.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTWWlGdWZOvs"
   },
   "outputs": [],
   "source": [
    "!mkdir model\n",
    "!tensorflowjs_converter --input_format keras keras.h5 model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKYxE2MEB6LV"
   },
   "source": [
    "# Compromimos y descargamos el modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "865-t79uaB63"
   },
   "outputs": [],
   "source": [
    "!cp class_names.txt model/class_names.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLC-MzW8ZXTa"
   },
   "outputs": [],
   "source": [
    "!zip -r model.zip model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vfPR03xZZeD"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('model.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vx2klIvlpCN8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Demo_Red_Neuronal.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
